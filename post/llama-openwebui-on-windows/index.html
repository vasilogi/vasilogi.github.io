<!doctype html>
<html lang="en-us"><head>
    <title>Ataraxia</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="" />

    
    
    
    <link rel="stylesheet" href="../../css/theme.min.css">

    
    
    

    
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-SKJNMNQZ5R"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-SKJNMNQZ5R');
        }
      </script>
    
  


</head>
<body>
        <div id="content" class="mx-auto"><header class="container mt-sm-5 mt-4 mb-4 mt-xs-1">
    <div class="row">
        
        <div class="col-sm-4 col-12 text-sm-right text-center pt-sm-4">
            <a href="../../" class="text-decoration-none">
                <img id="home-image" class="rounded-circle"
                    
                        
                            src="../../images/profile.jpg"
                        
                    
                />
            </a>
        </div>
        <div class="col-sm-8 col-12 text-sm-left text-center">
        
            <h2 class="m-0 mb-2 mt-4">
                <a href="../../" class="text-decoration-none">
                    
                        Ioannis Vasilopoulos
                    
                </a>
            </h2>
            <p class="text-muted mb-1">
                
                    AI Solutions Engineer
                
            </p>
            <ul id="nav-links" class="list-inline mb-2">
                
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../about/" title="About">About</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../post/" title="Posts">Posts</a>
                    </li>
                
                    <li class="list-inline-item">
                        <a class="badge badge-white " href="../../categories/" title="Categories">Categories</a>
                    </li>
                
            </ul>
            <ul id="nav-social" class="list-inline">
                
                    <li class="list-inline-item mr-3">
                        <a href="http://github.com/vasilogi" target="_blank">
                            <i class="fab fa-github fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="https://www.linkedin.com/in/vasilogi/" target="_blank">
                            <i class="fab fa-linkedin-in fa-1x text-muted"></i>
                        </a>
                    </li>
                
                    <li class="list-inline-item mr-3">
                        <a href="mailto:iovannis@outlook.com" target="_blank">
                            <i class="fas fa-at fa-1x text-muted"></i>
                        </a>
                    </li>
                
            </ul>
        </div>
    </div>
    <hr />
</header>
<div class="container">
    <div class="pl-sm-2">
        <div class="mb-3">
            <h3 class="mb-0">Installing Ollama on Windows and Setting Up Open WebUI</h3>
            
            <small class="text-muted">Published December 29, 2024</small>
        </div>

        <article>
            <p>This article demonstrates how to install <a href="https://ollama.com/">Ollama</a> and <a href="https://github.com/open-webui/open-webui">Open WebUI</a> on your local machine to self-host and interact with Generative AI models.</p>
<p>For optimal performance, it&rsquo;s recommended to install Ollama natively on your machine. Download the installer from the official <a href="https://ollama.com/">website</a> and follow the installation instructions. Once installed, launch Ollama and let it run in the background. Ollama provides a Command Line Interface (CLI) that enables you to manage compatible models, including pulling/pushing from a registry, viewing installed models, and creating new ones. You can view all available CLI commands by executing:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama help
</span></span></code></pre></div><p>To verify if Ollama is running properly, visit <code>http://127.0.0.1:11434</code> or execute:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/version
</span></span></code></pre></div><p>You can find Ollama-compatible models in their <a href="https://ollama.com/search">library</a> . For example, toinstall <code>llama3</code>, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama pull llama3:8b
</span></span></code></pre></div><blockquote>
<p>Choose models based on your machine&rsquo;s specifications. I prefer using smaller models since my development environment has a graphics card that isn&rsquo;t compatible with Ollama (as of the current version).</p>
</blockquote>
<blockquote>
<p>You can find a list of Ollama-compatible GPUs <a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md">here</a>.</p>
</blockquote>
<p>While you can interact directly with installed models using the CLI, <a href="https://github.com/open-webui/open-webui">Open WebUI</a> offers a more user-friendly alternative with many more capabilities. Their <a href="https://docs.openwebui.com/">documentation</a> provides detailed information on getting started. The recommended method is to run Open WebUI using Docker. The default Docker command is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker run -d -p 3000:8080 --add-host<span style="color:#f92672">=</span>host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><blockquote>
<p><a href="https://docs.openwebui.com/troubleshooting/connection-error">Server Connectivity Issues</a> are common with Open WebUI. In my case, port 3000 was already in use by another service, so I had to change it to 3030.</p>
</blockquote>
<blockquote>
<p>Both Ollama and Open WebUI are rapidly evolving technologies. To avoid potential compatibility issues, it&rsquo;s recommended to use a specific version of Open WebUI rather than the latest one.</p>
</blockquote>
<p>For easier solution maintenance, consider using <code>docker-compose</code>. The recommended Docker run command, mentioned above, can be converted to this docker-compose configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-YAML" data-lang="YAML"><span style="display:flex;"><span><span style="color:#f92672">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">open-webui</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">ghcr.io/open-webui/open-webui:v0.4.8</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">container_name</span>: <span style="color:#ae81ff">open-webui</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;3030:8080&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">extra_hosts</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;host.docker.internal:host-gateway&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">open-webui:/app/backend/data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">restart</span>: <span style="color:#ae81ff">always</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">open-webui</span>:
</span></span></code></pre></div><p>To start the container, simply run <code>docker compose up -d</code>. Ensure Ollama is running before launching the container.</p>

        </article>
    </div>

    

            </div>
        </div><footer class="text-center pb-1">
    <small class="text-muted">
        
            &copy; 2024, Ioannis Vasilopoulos
        
        <br>
        Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a>
        and <a href="https://github.com/austingebauer/devise" target="_blank">Devise</a>
    </small>
</footer>
</body>
</html>
